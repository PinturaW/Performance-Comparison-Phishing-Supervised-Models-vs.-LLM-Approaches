import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
import pandas as pd
import os
import csv
import time
from urllib.parse import urlparse
import random

class HTMLAnalyzerComplete:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.redirects_count = 0
    
    def load_urls_from_file(self, filename):
        """à¸­à¹ˆà¸²à¸™ URLs à¸ˆà¸²à¸à¹„à¸Ÿà¸¥à¹Œ"""
        try:
            # à¹ƒà¸Šà¹‰ Python standard file reading à¹à¸—à¸™ window.fs.readFile
            with open(filename, 'r', encoding='utf-8') as file:
                content = file.read()
                urls = [url.strip() for url in content.split('\n') if url.strip()]
                return urls
        except FileNotFoundError:
            print(f"âŒ Error: File '{filename}' not found")
            return []
        except Exception as e:
            print(f"âŒ Error reading file {filename}: {str(e)}")
            return []
    
    def generate_filename_from_url(self, url):
        """à¸ªà¸£à¹‰à¸²à¸‡à¸Šà¸·à¹ˆà¸­à¹„à¸Ÿà¸¥à¹Œà¸ˆà¸²à¸ URL"""
        try:
            parsed = urlparse(url)
            domain = parsed.netloc
            
            # à¸¥à¸š www. à¸«à¸²à¸à¸¡à¸µ
            if domain.startswith('www.'):
                domain = domain[4:]
            
            # à¹à¸—à¸™à¸—à¸µà¹ˆà¸•à¸±à¸§à¸­à¸±à¸à¸©à¸£à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¹ƒà¸Šà¹ˆà¸•à¸±à¸§à¸­à¸±à¸à¸©à¸£à¹à¸¥à¸°à¸•à¸±à¸§à¹€à¸¥à¸‚à¸”à¹‰à¸§à¸¢ underscore
            filename = re.sub(r'[^a-zA-Z0-9]', '_', domain)
            
            # à¸¥à¸š underscore à¸—à¸µà¹ˆà¸‹à¹‰à¸³à¸•à¸´à¸”à¸à¸±à¸™
            filename = re.sub(r'_+', '_', filename)
            
            # à¸¥à¸š underscore à¸—à¸µà¹ˆà¸‚à¸¶à¹‰à¸™à¸•à¹‰à¸™à¹à¸¥à¸°à¸¥à¸‡à¸—à¹‰à¸²à¸¢
            filename = filename.strip('_')
            
            # à¸–à¹‰à¸²à¸Šà¸·à¹ˆà¸­à¸¢à¸²à¸§à¹€à¸à¸´à¸™à¹„à¸› à¹ƒà¸«à¹‰à¸•à¸±à¸”à¹ƒà¸«à¹‰à¹€à¸«à¸¥à¸·à¸­ 50 à¸•à¸±à¸§à¸­à¸±à¸à¸©à¸£
            if len(filename) > 50:
                filename = filename[:50]
            
            return filename
        except Exception as e:
            print(f"âŒ Error generating filename for {url}: {str(e)}")
            # à¸ªà¸³à¸£à¸­à¸‡à¹ƒà¸Šà¹‰à¹€à¸¥à¸‚à¸ªà¸¸à¹ˆà¸¡
            return f"unknown_{random.randint(1000, 9999)}"
    
    def analyze_single_url(self, url, filename, list_type='white'):
        """à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ URL à¹à¸¥à¸°à¹à¸ªà¸”à¸‡à¸œà¸¥à¹à¸šà¸šà¸šà¸£à¸£à¸—à¸±à¸”à¸•à¹ˆà¸­à¸šà¸£à¸£à¸—à¸±à¸”"""
        try:
            print(f"ğŸ” Analyzing: {url}")
            
            response = self.get_html_with_redirect_count(url)
            if not response:
                print(f"âŒ Failed to fetch {url}")
                return None
            
            soup = BeautifulSoup(response.text, 'lxml')
            print(f"ğŸ“„ HTML content length: {len(response.text)} characters")
            
            # à¸ªà¸£à¹‰à¸²à¸‡ string à¹à¸šà¸šà¸šà¸£à¸£à¸—à¸±à¸”à¸•à¹ˆà¸­à¸šà¸£à¸£à¸—à¸±à¸”
            result_string = self.create_line_by_line_string(soup, response.url)
            
            # à¹€à¸•à¸£à¸µà¸¢à¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ªà¸³à¸«à¸£à¸±à¸š CSV
            result = {
                'data_string': result_string
            }
            
            # à¸šà¸±à¸™à¸—à¸¶à¸à¹„à¸Ÿà¸¥à¹Œ
            self.save_to_csv(result, filename, list_type)
            
            print(f"âœ… Analysis completed!")
            return result
            
        except Exception as e:
            print(f"âŒ Error analyzing {url}: {str(e)}")
            return None
    
    def batch_analyze_urls(self, urls, list_type='white', delay=1):
        """à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ URLs à¹€à¸›à¹‡à¸™à¸à¸¥à¸¸à¹ˆà¸¡"""
        print(f"\nğŸš€ Starting batch analysis for {len(urls)} URLs ({list_type}list)")
        print(f"ğŸ“ Files will be saved in: csv_training/{list_type}/")
        
        success_count = 0
        failed_count = 0
        
        for i, url in enumerate(urls, 1):
            print(f"\n{'='*60}")
            print(f"ğŸ“Š Progress: {i}/{len(urls)} ({(i/len(urls)*100):.1f}%)")
            
            # à¸ªà¸£à¹‰à¸²à¸‡à¸Šà¸·à¹ˆà¸­à¹„à¸Ÿà¸¥à¹Œ
            filename = self.generate_filename_from_url(url)
            
            try:
                result = self.analyze_single_url(url, filename, list_type)
                if result:
                    success_count += 1
                    print(f"âœ… Success: {url}")
                else:
                    failed_count += 1
                    print(f"âŒ Failed: {url}")
            except Exception as e:
                failed_count += 1
                print(f"âŒ Error with {url}: {str(e)}")
            
            # Delay à¹€à¸à¸·à¹ˆà¸­à¹„à¸¡à¹ˆà¹ƒà¸«à¹‰à¸ªà¹ˆà¸‡ request à¹€à¸£à¹‡à¸§à¹€à¸à¸´à¸™à¹„à¸›
            if i < len(urls):  # à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡ delay à¸«à¸¥à¸±à¸‡à¸ˆà¸²à¸ URL à¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢
                time.sleep(delay)
        
        print(f"\n{'='*60}")
        print(f"ğŸ‰ Batch analysis completed!")
        print(f"âœ… Successful: {success_count}")
        print(f"âŒ Failed: {failed_count}")
        
        # Fixed division by zero error
        total_processed = success_count + failed_count
        if total_processed > 0:
            success_rate = (success_count / total_processed) * 100
            print(f"ğŸ“Š Success rate: {success_rate:.1f}%")
        else:
            print("ğŸ“Š No URLs were processed")
        
        return {
            'success_count': success_count,
            'failed_count': failed_count,
            'total': len(urls)
        }
    
    def process_training_data(self):
        """à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ training à¸—à¸±à¹‰à¸‡ blacklist à¹à¸¥à¸° whitelist"""
        print("ğŸ¯ Starting training data processing...")
        
        # à¸­à¹ˆà¸²à¸™ blacklist URLs
        print("\nğŸ“‹ Loading blacklist URLs...")
        blacklist_urls = self.load_urls_from_file('paste-2.txt')
        print(f"ğŸ“Š Found {len(blacklist_urls)} blacklist URLs")
        
        # à¸­à¹ˆà¸²à¸™ whitelist URLs  
        print("\nğŸ“‹ Loading whitelist URLs...")
        whitelist_urls = self.load_urls_from_file('paste-3.txt')
        print(f"ğŸ“Š Found {len(whitelist_urls)} whitelist URLs")
        
        # à¸ˆà¸³à¸à¸±à¸”à¸ˆà¸³à¸™à¸§à¸™ URLs à¸•à¸²à¸¡à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£ (400 URLs à¸ªà¸³à¸«à¸£à¸±à¸šà¹à¸•à¹ˆà¸¥à¸°à¸›à¸£à¸°à¹€à¸ à¸—)
        blacklist_urls = blacklist_urls[:400]
        whitelist_urls = whitelist_urls[:400]
        
        print(f"\nğŸ¯ Processing {len(blacklist_urls)} blacklist URLs and {len(whitelist_urls)} whitelist URLs")
        
        # à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥ blacklist
        print(f"\nğŸ”´ Processing BLACKLIST URLs...")
        blacklist_results = self.batch_analyze_urls(blacklist_urls, 'black', delay=2)
        
        # à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥ whitelist
        print(f"\nğŸ”µ Processing WHITELIST URLs...")
        whitelist_results = self.batch_analyze_urls(whitelist_urls, 'white', delay=2)
        
        # à¸ªà¸£à¸¸à¸›à¸œà¸¥à¸£à¸§à¸¡
        print(f"\n{'='*80}")
        print(f"ğŸ‰ TRAINING DATA PROCESSING COMPLETED!")
        print(f"{'='*80}")
        print(f"ğŸ”´ BLACKLIST Results:")
        print(f"   âœ… Successful: {blacklist_results['success_count']}")
        print(f"   âŒ Failed: {blacklist_results['failed_count']}")
        
        # Fixed division by zero error for blacklist
        if blacklist_results['total'] > 0:
            blacklist_rate = (blacklist_results['success_count'] / blacklist_results['total'] * 100)
            print(f"   ğŸ“Š Success rate: {blacklist_rate:.1f}%")
        else:
            print("   ğŸ“Š Success rate: N/A (no URLs processed)")
        
        print(f"\nğŸ”µ WHITELIST Results:")
        print(f"   âœ… Successful: {whitelist_results['success_count']}")
        print(f"   âŒ Failed: {whitelist_results['failed_count']}")
        
        # Fixed division by zero error for whitelist
        if whitelist_results['total'] > 0:
            whitelist_rate = (whitelist_results['success_count'] / whitelist_results['total'] * 100)
            print(f"   ğŸ“Š Success rate: {whitelist_rate:.1f}%")
        else:
            print("   ğŸ“Š Success rate: N/A (no URLs processed)")
        
        total_success = blacklist_results['success_count'] + whitelist_results['success_count']
        total_processed = blacklist_results['total'] + whitelist_results['total']
        
        print(f"\nğŸ“Š OVERALL STATISTICS:")
        print(f"   ğŸ¯ Total URLs processed: {total_processed}")
        print(f"   âœ… Total successful: {total_success}")
        
        # Fixed division by zero error for overall
        if total_processed > 0:
            overall_rate = (total_success / total_processed * 100)
            print(f"   ğŸ“Š Overall success rate: {overall_rate:.1f}%")
        else:
            print("   ğŸ“Š Overall success rate: N/A (no URLs processed)")
            
        print(f"   ğŸ“ Files saved in: csv_training/black/ and csv_training/white/")
        
        return {
            'blacklist': blacklist_results,
            'whitelist': whitelist_results,
            'total_success': total_success,
            'total_processed': total_processed
        }
    
    def create_line_by_line_string(self, soup, url):
        """à¸ªà¸£à¹‰à¸²à¸‡ string à¹à¸šà¸šà¸šà¸£à¸£à¸—à¸±à¸”à¸•à¹ˆà¸­à¸šà¸£à¸£à¸—à¸±à¸” à¸„à¸£à¸šà¸–à¹‰à¸§à¸™à¸—à¸¸à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥"""
        lines = []
        
        # à¹€à¸à¸´à¹ˆà¸¡ URL à¹à¸¥à¸° Domain
        lines.append(f"URL: {url}")
        lines.append(f"Domain: {self.extract_domain(url)}")
        
        # à¹€à¸à¸´à¹ˆà¸¡ Title
        title = self.extract_title(soup)
        lines.append(f"Title: {title}")
        
        # à¹€à¸à¸´à¹ˆà¸¡ Meta tags à¹à¸•à¹ˆà¸¥à¸°à¸•à¸±à¸§à¹€à¸›à¹‡à¸™à¸šà¸£à¸£à¸—à¸±à¸” - à¹„à¸¡à¹ˆà¸•à¸±à¸”à¹€à¸¥à¸¢
        meta_tags = soup.find_all('meta')
        
        print(f"\nğŸ“‹ Processing all {len(meta_tags)} meta tags:")
        
        for i, meta in enumerate(meta_tags, 1):
            if meta.get('name'):
                name_val = self.clean_value(meta.get('name'))
                content_val = self.clean_value(meta.get('content', ''))
                line = f"Meta_{name_val}: {content_val}"
                lines.append(line)
                
            elif meta.get('property'):
                prop_val = self.clean_value(meta.get('property'))
                content_val = self.clean_value(meta.get('content', ''))
                line = f"Meta_{prop_val}: {content_val}"
                lines.append(line)
                
            elif meta.get('http-equiv'):
                equiv_val = self.clean_value(meta.get('http-equiv'))
                content_val = self.clean_value(meta.get('content', ''))
                line = f"Meta_{equiv_val}: {content_val}"
                lines.append(line)
                
            elif meta.get('charset'):
                charset_val = self.clean_value(meta.get('charset'))
                line = f"Meta_charset: {charset_val}"
                lines.append(line)
                
            elif meta.get('itemprop'):
                item_val = self.clean_value(meta.get('itemprop'))
                content_val = self.clean_value(meta.get('content', ''))
                line = f"Meta_{item_val}: {content_val}"
                lines.append(line)
                
            else:
                # à¸ªà¸³à¸«à¸£à¸±à¸š attributes à¸­à¸·à¹ˆà¸™à¹†
                attrs_list = []
                for k, v in meta.attrs.items():
                    clean_k = self.clean_value(k)
                    clean_v = self.clean_value(v)
                    if clean_v:
                        attrs_list.append(f"{clean_k}={clean_v}")
                    else:
                        attrs_list.append(clean_k)
                
                if attrs_list:
                    line = f"Meta_{','.join(attrs_list)}:"
                    lines.append(line)
        
        # à¹€à¸à¸´à¹ˆà¸¡ Icon type
        icon_type = self.analyze_icon_type(soup, url)
        lines.append(f"Icon_type: {icon_type}")
        
        # à¸™à¸±à¸š script à¹à¸¥à¸° resources à¸•à¸²à¸¡à¸—à¸µà¹ˆà¸à¸³à¸«à¸™à¸”
        script_counts = self.count_all_scripts(soup, url)
        resource_counts = self.count_all_external_resources(soup, url)
        
        # à¹à¸¢à¸à¹€à¸›à¹‡à¸™à¸šà¸£à¸£à¸—à¸±à¸” Number of à¸•à¹ˆà¸²à¸‡à¹†
        lines.append(f"Number_of_redirect: {self.redirects_count}")
        lines.append(f"Number_of_popup: {self.count_popups(soup)}")
        lines.append(f"Number_of_script: {script_counts['total_scripts']}")
        lines.append(f"Number_of_src_script: {script_counts['src_scripts']}")
        lines.append(f"Number_of_inline_script: {script_counts['inline_scripts']}")
        lines.append(f"Number_of_external_script: {script_counts['external_scripts']}")
        lines.append(f"Number_of_external_resources: {resource_counts['total_external']}")
        
        # à¸£à¸§à¸¡à¹€à¸›à¹‡à¸™ string à¹€à¸”à¸µà¸¢à¸§à¸”à¹‰à¸§à¸¢ newline
        result_string = '\n'.join(lines)
        
        print(f"ğŸ“Š Final statistics:")
        print(f"   â€¢ Total lines: {len(lines)}")
        print(f"   â€¢ Meta tags: {len(meta_tags)}")
        print(f"   â€¢ Scripts: {script_counts['total_scripts']} (Src: {script_counts['src_scripts']}, Inline: {script_counts['inline_scripts']}, External: {script_counts['external_scripts']})")
        print(f"   â€¢ External resources: {resource_counts['total_external']}")
        print(f"   â€¢ String length: {len(result_string)} characters")
        
        return result_string
    
    def get_html_with_redirect_count(self, url):
        """à¸”à¸¶à¸‡ HTML à¹à¸¥à¸°à¸™à¸±à¸š redirect"""
        self.redirects_count = 0
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
            
            response = self.session.get(url, timeout=30, allow_redirects=False, headers=headers)
            
            while response.status_code in [301, 302, 303, 307, 308]:
                self.redirects_count += 1
                redirect_url = response.headers.get('Location')
                if redirect_url:
                    if not redirect_url.startswith('http'):
                        redirect_url = urljoin(url, redirect_url)
                    response = self.session.get(redirect_url, timeout=30, allow_redirects=False, headers=headers)
                else:
                    break
                
                # à¸›à¹‰à¸­à¸‡à¸à¸±à¸™ infinite redirect
                if self.redirects_count > 10:
                    print("âš ï¸ Too many redirects, stopping...")
                    break
            
            if response.status_code == 200:
                return response
            else:
                response.raise_for_status()
                
        except requests.RequestException as e:
            print(f"Failed to fetch {url}: {str(e)}")
            return None
    
    def extract_domain(self, url):
        return urlparse(url).netloc
    
    def extract_title(self, soup):
        title_tag = soup.find('title')
        return title_tag.get_text().strip() if title_tag else ""
    
    def clean_value(self, value):
        """à¸—à¸³à¸„à¸§à¸²à¸¡à¸ªà¸°à¸­à¸²à¸”à¸„à¹ˆà¸²à¹‚à¸”à¸¢à¹„à¸¡à¹ˆà¸•à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥"""
        if value is None:
            return ""
        # à¸¥à¸šà¹€à¸‰à¸à¸²à¸°à¸•à¸±à¸§à¸­à¸±à¸à¸©à¸£à¸—à¸µà¹ˆà¸­à¸²à¸ˆà¸—à¸³à¹ƒà¸«à¹‰ CSV à¹€à¸ªà¸µà¸¢
        clean_val = str(value).replace('"', '').replace("'", '').replace('\n', ' ').replace('\r', ' ').strip()
        clean_val = re.sub(r'\s+', ' ', clean_val)
        return clean_val
    
    def analyze_icon_type(self, soup, base_url):
        """à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ icon type"""
        icon_selectors = [
            'link[rel*="icon"]',
            'link[rel="shortcut icon"]',
            'link[rel="apple-touch-icon"]',
            'link[rel="apple-touch-icon-precomposed"]',
            'link[rel="favicon"]', 
            'link[rel="mask-icon"]'
        ]
        
        base_domain = urlparse(base_url).netloc
        internal_count = 0
        external_count = 0
        
        for selector in icon_selectors:
            for link in soup.select(selector):
                href = link.get('href')
                if href:
                    absolute_url = urljoin(base_url, href)
                    icon_domain = urlparse(absolute_url).netloc
                    
                    if icon_domain == base_domain or not icon_domain:
                        internal_count += 1
                    else:
                        external_count += 1
        
        if internal_count > 0 and external_count == 0:
            return 'internal'
        elif external_count > 0 and internal_count == 0:
            return 'external'
        elif internal_count > 0 and external_count > 0:
            return 'mixed'
        else:
            return 'none'
    
    def count_all_scripts(self, soup, base_url):
        """à¸™à¸±à¸š script à¸—à¸¸à¸à¸›à¸£à¸°à¹€à¸ à¸—à¸•à¸²à¸¡à¸„à¸³à¸ˆà¸³à¸à¸±à¸”à¸„à¸§à¸²à¸¡à¸—à¸µà¹ˆà¸Šà¸±à¸”à¹€à¸ˆà¸™"""
        all_scripts = soup.find_all('script')
        base_domain = urlparse(base_url).netloc
        
        src_scripts = []
        inline_scripts = []
        external_scripts = []
        
        for i, script in enumerate(all_scripts, 1):
            src = script.get('src')
            
            if src:
                # Script à¸—à¸µà¹ˆà¸¡à¸µ src attribute
                src_scripts.append(script)
                
                # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¹€à¸›à¹‡à¸™ external à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
                if src.startswith('http'):
                    # URL à¹€à¸•à¹‡à¸¡
                    script_domain = urlparse(src).netloc
                    if script_domain != base_domain:
                        external_scripts.append(script)
                elif src.startswith('//'):
                    # Protocol-relative URL
                    script_domain = urlparse('http:' + src).netloc
                    if script_domain != base_domain:
                        external_scripts.append(script)
            else:
                # Script à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸¡à¸µ src (inline)
                inline_scripts.append(script)
        
        result = {
            'total_scripts': len(all_scripts),
            'src_scripts': len(src_scripts),
            'inline_scripts': len(inline_scripts),
            'external_scripts': len(external_scripts)
        }
        
        return result
    
    def count_all_external_resources(self, soup, base_url):
        """à¸™à¸±à¸š external resources à¸—à¸¸à¸à¸›à¸£à¸°à¹€à¸ à¸—"""
        base_domain = urlparse(base_url).netloc
        external_resources = []
        
        # à¸£à¸²à¸¢à¸à¸²à¸£ tags à¹à¸¥à¸° attributes à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š
        resource_tags = [
            ('script', 'src'),
            ('link', 'href'),
            ('img', 'src'),
            ('iframe', 'src'),
            ('embed', 'src'),
            ('object', 'data'),
            ('source', 'src'),
            ('track', 'src'),
            ('video', 'src'),
            ('audio', 'src'),
            ('input', 'src'),  # à¸ªà¸³à¸«à¸£à¸±à¸š input type="image"
            ('frame', 'src'),
            ('applet', 'archive'),
            ('area', 'href'),
            ('base', 'href')
        ]
        
        external_by_type = {}
        
        for tag_name, attr_name in resource_tags:
            count = 0
            for element in soup.find_all(tag_name):
                src = element.get(attr_name)
                if src:
                    # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¹€à¸›à¹‡à¸™ external à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
                    is_external = False
                    
                    if src.startswith('http'):
                        # URL à¹€à¸•à¹‡à¸¡
                        src_domain = urlparse(src).netloc
                        if src_domain != base_domain:
                            is_external = True
                    elif src.startswith('//'):
                        # Protocol-relative URL
                        src_domain = urlparse('http:' + src).netloc
                        if src_domain != base_domain:
                            is_external = True
                    
                    if is_external:
                        external_resources.append({
                            'tag': tag_name,
                            'attr': attr_name,
                            'url': src
                        })
                        count += 1
            
            if count > 0:
                external_by_type[tag_name] = count
        
        result = {
            'total_external': len(external_resources),
            'by_type': external_by_type,
            'details': external_resources
        }
        
        return result
    
    def count_popups(self, soup):
        """à¸™à¸±à¸š popup patterns"""
        popup_patterns = [
            r'window\.open\s*\(',
            r'alert\s*\(',
            r'confirm\s*\(',
            r'prompt\s*\(',
            r'\.modal\s*\(',
            r'showModal',
            r'openPopup',
            r'\.popup\s*\(',
            r'\.dialog\s*\('
        ]
        
        popup_count = 0
        for script in soup.find_all('script'):
            if script.string:
                for pattern in popup_patterns:
                    matches = re.findall(pattern, script.string, re.IGNORECASE)
                    popup_count += len(matches)
        
        return popup_count
    
    def save_to_csv(self, data, filename, list_type):
        """à¸šà¸±à¸™à¸—à¸¶à¸à¹€à¸›à¹‡à¸™ CSV à¹‚à¸”à¸¢à¹„à¸¡à¹ˆà¸•à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥"""
        folder_path = f'csv_training/{list_type}'
        os.makedirs(folder_path, exist_ok=True)
        
        if not filename.endswith('.csv'):
            filename += '.csv'
        
        filepath = f'{folder_path}/{filename}'
        
        # à¸šà¸±à¸™à¸—à¸¶à¸à¹€à¸›à¹‡à¸™ CSV
        df = pd.DataFrame([data])
        df.to_csv(filepath, index=False, encoding='utf-8', quoting=csv.QUOTE_MINIMAL)
        
        print(f"ğŸ’¾ Saved to: {filepath}")
        
        # à¹à¸ªà¸”à¸‡à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡ string
        preview = data['data_string'][:200] + "..." if len(data['data_string']) > 200 else data['data_string']
        print(f"ğŸ“‹ Data preview: {preview}")
        
        # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸„à¸§à¸²à¸¡à¸ªà¸¡à¸šà¸¹à¸£à¸“à¹Œà¸‚à¸­à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥
        lines = data['data_string'].split('\n')
        print(f"ğŸ“Š Lines: {len(lines)}, Length: {len(data['data_string'])} chars")

# à¸à¸²à¸£à¹ƒà¸Šà¹‰à¸‡à¸²à¸™
if __name__ == "__main__":
    analyzer = HTMLAnalyzerComplete()
    
    # à¹€à¸£à¸µà¸¢à¸à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ training
    print("ğŸ¯ Starting Training Data Processing...")
    results = analyzer.process_training_data()
    
    print(f"\nğŸ‰ All done! Check your csv_training folder for results.")