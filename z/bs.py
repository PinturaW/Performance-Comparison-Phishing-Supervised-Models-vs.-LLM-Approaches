import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
import pandas as pd
import os
import csv

class HTMLAnalyzerComplete:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.redirects_count = 0
    
    def analyze_single_url(self, url, filename, list_type='white'):
        """à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ URL à¹à¸¥à¸°à¹à¸ªà¸”à¸‡à¸œà¸¥à¹à¸šà¸šà¸šà¸£à¸£à¸—à¸±à¸”à¸•à¹ˆà¸­à¸šà¸£à¸£à¸—à¸±à¸”"""
        try:
            print(f"ğŸ” Analyzing: {url}")
            
            response = self.get_html_with_redirect_count(url)
            if not response:
                print(f"âŒ Failed to fetch {url}")
                return None
            
            soup = BeautifulSoup(response.text, 'lxml')
            print(f"ğŸ“„ HTML content length: {len(response.text)} characters")
            
            # à¸ªà¸£à¹‰à¸²à¸‡ string à¹à¸šà¸šà¸šà¸£à¸£à¸—à¸±à¸”à¸•à¹ˆà¸­à¸šà¸£à¸£à¸—à¸±à¸”
            result_string = self.create_line_by_line_string(soup, response.url)
            
            # à¹€à¸•à¸£à¸µà¸¢à¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ªà¸³à¸«à¸£à¸±à¸š CSV
            result = {
                'data_string': result_string
            }
            
            # à¸šà¸±à¸™à¸—à¸¶à¸à¹„à¸Ÿà¸¥à¹Œ
            self.save_to_csv(result, filename, list_type)
            
            print(f"âœ… Analysis completed!")
            return result
            
        except Exception as e:
            print(f"âŒ Error analyzing {url}: {str(e)}")
            return None
    
    def create_line_by_line_string(self, soup, url):
        """à¸ªà¸£à¹‰à¸²à¸‡ string à¹à¸šà¸šà¸šà¸£à¸£à¸—à¸±à¸”à¸•à¹ˆà¸­à¸šà¸£à¸£à¸—à¸±à¸” à¸„à¸£à¸šà¸–à¹‰à¸§à¸™à¸—à¸¸à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥"""
        lines = []
        
        # à¹€à¸à¸´à¹ˆà¸¡ URL à¹à¸¥à¸° Domain
        lines.append(f"URL: {url}")
        lines.append(f"Domain: {self.extract_domain(url)}")
        
        # à¹€à¸à¸´à¹ˆà¸¡ Title
        title = self.extract_title(soup)
        lines.append(f"Title: {title}")
        
        # à¹€à¸à¸´à¹ˆà¸¡ Meta tags à¹à¸•à¹ˆà¸¥à¸°à¸•à¸±à¸§à¹€à¸›à¹‡à¸™à¸šà¸£à¸£à¸—à¸±à¸” - à¹„à¸¡à¹ˆà¸•à¸±à¸”à¹€à¸¥à¸¢
        meta_tags = soup.find_all('meta')
        
        print(f"\nğŸ“‹ Processing all {len(meta_tags)} meta tags:")
        
        for i, meta in enumerate(meta_tags, 1):
            if meta.get('name'):
                name_val = self.clean_value(meta.get('name'))
                content_val = self.clean_value(meta.get('content', ''))
                line = f"Meta_{name_val}: {content_val}"
                lines.append(line)
                
            elif meta.get('property'):
                prop_val = self.clean_value(meta.get('property'))
                content_val = self.clean_value(meta.get('content', ''))
                line = f"Meta_{prop_val}: {content_val}"
                lines.append(line)
                
            elif meta.get('http-equiv'):
                equiv_val = self.clean_value(meta.get('http-equiv'))
                content_val = self.clean_value(meta.get('content', ''))
                line = f"Meta_{equiv_val}: {content_val}"
                lines.append(line)
                
            elif meta.get('charset'):
                charset_val = self.clean_value(meta.get('charset'))
                line = f"Meta_charset: {charset_val}"
                lines.append(line)
                
            elif meta.get('itemprop'):
                item_val = self.clean_value(meta.get('itemprop'))
                content_val = self.clean_value(meta.get('content', ''))
                line = f"Meta_{item_val}: {content_val}"
                lines.append(line)
                
            else:
                # à¸ªà¸³à¸«à¸£à¸±à¸š attributes à¸­à¸·à¹ˆà¸™à¹†
                attrs_list = []
                for k, v in meta.attrs.items():
                    clean_k = self.clean_value(k)
                    clean_v = self.clean_value(v)
                    if clean_v:
                        attrs_list.append(f"{clean_k}={clean_v}")
                    else:
                        attrs_list.append(clean_k)
                
                if attrs_list:
                    line = f"Meta_{','.join(attrs_list)}:"
                    lines.append(line)
        
        # à¹€à¸à¸´à¹ˆà¸¡ Icon type
        icon_type = self.analyze_icon_type(soup, url)
        lines.append(f"Icon_type: {icon_type}")
        
        # à¸™à¸±à¸š script à¹à¸¥à¸° external resources à¹à¸¢à¸à¸à¸±à¸™
        script_counts = self.count_all_scripts(soup, url)
        external_non_script_counts = self.count_external_non_script_resources(soup, url)
        
        # à¹à¸¢à¸à¹€à¸›à¹‡à¸™à¸šà¸£à¸£à¸—à¸±à¸” Number of à¸•à¹ˆà¸²à¸‡à¹† à¸•à¸²à¸¡à¸—à¸µà¹ˆà¸„à¸¸à¸“à¸•à¹‰à¸­à¸‡à¸à¸²à¸£
        lines.append(f"Number_of_redirect: {self.redirects_count}")
        lines.append(f"Number_of_popup: {self.count_popups(soup)}")
        lines.append(f"Number_of_script: {script_counts['total_scripts']}")
        lines.append(f"Number_of_src_script: {script_counts['src_scripts']}")
        lines.append(f"Number_of_inline_script: {script_counts['inline_scripts']}")
        lines.append(f"Number_of_external_src_script: {script_counts['external_scripts']}")  # à¹à¸¢à¸à¸­à¸­à¸à¸¡à¸²à¸Šà¸±à¸”à¹€à¸ˆà¸™
        lines.append(f"Number_of_external_src: {external_non_script_counts['total_external']}")  # resource à¸­à¸·à¹ˆà¸™à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¹ƒà¸Šà¹ˆ script
        
        # à¸£à¸§à¸¡à¹€à¸›à¹‡à¸™ string à¹€à¸”à¸µà¸¢à¸§à¸”à¹‰à¸§à¸¢ newline
        result_string = '\n'.join(lines)
        
        print(f"ğŸ“Š Final statistics:")
        print(f"   â€¢ Total lines: {len(lines)}")
        print(f"   â€¢ Meta tags: {len(meta_tags)}")
        print(f"   â€¢ Scripts: {script_counts['total_scripts']} (Src: {script_counts['src_scripts']}, Inline: {script_counts['inline_scripts']}, External: {script_counts['external_scripts']})")
        print(f"   â€¢ External non-script resources: {external_non_script_counts['total_external']}")
        print(f"   â€¢ String length: {len(result_string)} characters")
        
        return result_string
    
    def get_html_with_redirect_count(self, url):
        """à¸”à¸¶à¸‡ HTML à¹à¸¥à¸°à¸™à¸±à¸š redirect"""
        self.redirects_count = 0
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
            
            response = self.session.get(url, timeout=30, allow_redirects=False, headers=headers)
            
            while response.status_code in [301, 302, 303, 307, 308]:
                self.redirects_count += 1
                redirect_url = response.headers.get('Location')
                if redirect_url:
                    if not redirect_url.startswith('http'):
                        redirect_url = urljoin(url, redirect_url)
                    response = self.session.get(redirect_url, timeout=30, allow_redirects=False, headers=headers)
                else:
                    break
                
                # à¸›à¹‰à¸­à¸‡à¸à¸±à¸™ infinite redirect
                if self.redirects_count > 10:
                    print("âš ï¸ Too many redirects, stopping...")
                    break
            
            if response.status_code == 200:
                return response
            else:
                response.raise_for_status()
                
        except requests.RequestException as e:
            print(f"Failed to fetch {url}: {str(e)}")
            return None
    
    def extract_domain(self, url):
        return urlparse(url).netloc
    
    def extract_title(self, soup):
        title_tag = soup.find('title')
        return title_tag.get_text().strip() if title_tag else ""
    
    def clean_value(self, value):
        """à¸—à¸³à¸„à¸§à¸²à¸¡à¸ªà¸°à¸­à¸²à¸”à¸„à¹ˆà¸²à¹‚à¸”à¸¢à¹„à¸¡à¹ˆà¸•à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥"""
        if value is None:
            return ""
        # à¸¥à¸šà¹€à¸‰à¸à¸²à¸°à¸•à¸±à¸§à¸­à¸±à¸à¸©à¸£à¸—à¸µà¹ˆà¸­à¸²à¸ˆà¸—à¸³à¹ƒà¸«à¹‰ CSV à¹€à¸ªà¸µà¸¢
        clean_val = str(value).replace('"', '').replace("'", '').replace('\n', ' ').replace('\r', ' ').strip()
        clean_val = re.sub(r'\s+', ' ', clean_val)
        return clean_val
    
    def analyze_icon_type(self, soup, base_url):
        """à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ icon type"""
        icon_selectors = [
            'link[rel*="icon"]',
            'link[rel="shortcut icon"]',
            'link[rel="apple-touch-icon"]',
            'link[rel="apple-touch-icon-precomposed"]',
            'link[rel="favicon"]', 
            'link[rel="mask-icon"]'
        ]
        
        base_domain = urlparse(base_url).netloc
        internal_count = 0
        external_count = 0
        
        for selector in icon_selectors:
            for link in soup.select(selector):
                href = link.get('href')
                if href:
                    absolute_url = urljoin(base_url, href)
                    icon_domain = urlparse(absolute_url).netloc
                    
                    if icon_domain == base_domain or not icon_domain:
                        internal_count += 1
                    else:
                        external_count += 1
        
        if internal_count > 0 and external_count == 0:
            return 'internal'
        elif external_count > 0 and internal_count == 0:
            return 'external'
        elif internal_count > 0 and external_count > 0:
            return 'mixed'
        else:
            return 'none'
    
    def count_all_scripts(self, soup, base_url):
        """à¸™à¸±à¸š script à¸—à¸¸à¸à¸›à¸£à¸°à¹€à¸ à¸—à¸•à¸²à¸¡à¸„à¸³à¸ˆà¸³à¸à¸±à¸”à¸„à¸§à¸²à¸¡à¸—à¸µà¹ˆà¸Šà¸±à¸”à¹€à¸ˆà¸™"""
        all_scripts = soup.find_all('script')
        base_domain = urlparse(base_url).netloc
        
        src_scripts = []
        inline_scripts = []
        external_scripts = []
        
        print(f"\nğŸ“Š Complete Script Analysis:")
        print(f"   â€¢ Total script tags found: {len(all_scripts)}")
        
        for i, script in enumerate(all_scripts, 1):
            src = script.get('src')
            
            if src:
                # Script à¸—à¸µà¹ˆà¸¡à¸µ src attribute
                src_scripts.append(script)
                
                # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¹€à¸›à¹‡à¸™ external à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
                if src.startswith('http'):
                    # URL à¹€à¸•à¹‡à¸¡
                    script_domain = urlparse(src).netloc
                    if script_domain != base_domain:
                        external_scripts.append(script)
                        print(f"   {i:2d}. EXTERNAL SRC: {src}")
                    else:
                        print(f"   {i:2d}. INTERNAL SRC: {src}")
                elif src.startswith('//'):
                    # Protocol-relative URL
                    script_domain = urlparse('http:' + src).netloc
                    if script_domain != base_domain:
                        external_scripts.append(script)
                        print(f"   {i:2d}. EXTERNAL SRC: {src}")
                    else:
                        print(f"   {i:2d}. INTERNAL SRC: {src}")
                else:
                    # Relative URL - à¸–à¸·à¸­à¸§à¹ˆà¸²à¹€à¸›à¹‡à¸™ internal
                    print(f"   {i:2d}. INTERNAL SRC: {src}")
            else:
                # Script à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸¡à¸µ src (inline)
                inline_scripts.append(script)
                content_preview = ""
                if script.string:
                    content_preview = script.string.strip()[:50]
                    if len(script.string.strip()) > 50:
                        content_preview += "..."
                print(f"   {i:2d}. INLINE: {content_preview}")
        
        result = {
            'total_scripts': len(all_scripts),
            'src_scripts': len(src_scripts),
            'inline_scripts': len(inline_scripts),
            'external_scripts': len(external_scripts)
        }
        
        print(f"   ğŸ“Š Script Summary:")
        print(f"      â€¢ Total scripts: {result['total_scripts']}")
        print(f"      â€¢ With src: {result['src_scripts']}")
        print(f"      â€¢ Inline: {result['inline_scripts']}")
        print(f"      â€¢ External src: {result['external_scripts']}")
        
        return result
    
    def count_external_non_script_resources(self, soup, base_url):
        """à¸™à¸±à¸š external resources à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¹ƒà¸Šà¹ˆ script"""
        base_domain = urlparse(base_url).netloc
        external_resources = []
        
        # à¸£à¸²à¸¢à¸à¸²à¸£ tags à¹à¸¥à¸° attributes à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š (à¹„à¸¡à¹ˆà¸£à¸§à¸¡ script)
        resource_tags = [
            ('link', 'href'),
            ('img', 'src'),
            ('iframe', 'src'),
            ('embed', 'src'),
            ('object', 'data'),
            ('source', 'src'),
            ('track', 'src'),
            ('video', 'src'),
            ('audio', 'src'),
            ('input', 'src'),  # à¸ªà¸³à¸«à¸£à¸±à¸š input type="image"
            ('frame', 'src'),
            ('applet', 'archive'),
            ('area', 'href'),
            ('base', 'href')
        ]
        
        print(f"\nğŸŒ External Non-Script Resources Analysis:")
        
        external_by_type = {}
        
        for tag_name, attr_name in resource_tags:
            count = 0
            for element in soup.find_all(tag_name):
                src = element.get(attr_name)
                if src:
                    # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¹€à¸›à¹‡à¸™ external à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
                    is_external = False
                    
                    if src.startswith('http'):
                        # URL à¹€à¸•à¹‡à¸¡
                        src_domain = urlparse(src).netloc
                        if src_domain != base_domain:
                            is_external = True
                    elif src.startswith('//'):
                        # Protocol-relative URL
                        src_domain = urlparse('http:' + src).netloc
                        if src_domain != base_domain:
                            is_external = True
                    
                    if is_external:
                        external_resources.append({
                            'tag': tag_name,
                            'attr': attr_name,
                            'url': src
                        })
                        count += 1
                        print(f"   EXTERNAL {tag_name.upper()}: {src}")
            
            if count > 0:
                external_by_type[tag_name] = count
        
        print(f"   ğŸ“Š External Non-Script Resources by Type:")
        for tag_type, count in external_by_type.items():
            print(f"      â€¢ {tag_type}: {count}")
        
        result = {
            'total_external': len(external_resources),
            'by_type': external_by_type,
            'details': external_resources
        }
        
        print(f"   ğŸ“Š Total external non-script resources: {result['total_external']}")
        
        return result
    
    def count_popups(self, soup):
        """à¸™à¸±à¸š popup patterns"""
        popup_patterns = [
            r'window\.open\s*\(',
            r'alert\s*\(',
            r'confirm\s*\(',
            r'prompt\s*\(',
            r'\.modal\s*\(',
            r'showModal',
            r'openPopup',
            r'\.popup\s*\(',
            r'\.dialog\s*\('
        ]
        
        popup_count = 0
        for script in soup.find_all('script'):
            if script.string:
                for pattern in popup_patterns:
                    matches = re.findall(pattern, script.string, re.IGNORECASE)
                    popup_count += len(matches)
        
        return popup_count
    
    def save_to_csv(self, data, filename, list_type):
        """à¸šà¸±à¸™à¸—à¸¶à¸à¹€à¸›à¹‡à¸™ CSV à¹‚à¸”à¸¢à¹„à¸¡à¹ˆà¸•à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥"""
        folder_path = f'csv/{list_type}'
        os.makedirs(folder_path, exist_ok=True)
        
        if not filename.endswith('.csv'):
            filename += '.csv'
        
        filepath = f'{folder_path}/{filename}'
        
        # à¸šà¸±à¸™à¸—à¸¶à¸à¹€à¸›à¹‡à¸™ CSV
        df = pd.DataFrame([data])
        df.to_csv(filepath, index=False, encoding='utf-8', quoting=csv.QUOTE_MINIMAL)
        
        print(f"ğŸ’¾ Saved to: {filepath}")
        
        # à¹à¸ªà¸”à¸‡à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡ string
        print(f"\nğŸ“‹ Data String Preview (first 500 chars):")
        preview = data['data_string'][:500] + "..." if len(data['data_string']) > 500 else data['data_string']
        print(preview)
        
        # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸„à¸§à¸²à¸¡à¸ªà¸¡à¸šà¸¹à¸£à¸“à¹Œà¸‚à¸­à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥
        lines = data['data_string'].split('\n')
        print(f"\nğŸ“Š Data completeness check:")
        print(f"   â€¢ Total lines in output: {len(lines)}")
        print(f"   â€¢ String length: {len(data['data_string'])} characters")
        
        # à¹à¸ªà¸”à¸‡à¸šà¸£à¸£à¸—à¸±à¸”à¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢à¹€à¸à¸·à¹ˆà¸­à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š
        if lines:
            print(f"   â€¢ Last line: {lines[-1]}")

# à¸à¸²à¸£à¹ƒà¸Šà¹‰à¸‡à¸²à¸™
if __name__ == "__main__":
    analyzer = HTMLAnalyzerComplete()
    
    # à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ Shopee
    print("ğŸŸ¢ Analyzing Shopee...")
    result = analyzer.analyze_single_url("https://playnows.co/register", "playnows", "test")
    
    if result:
        print("\nâœ… Analysis completed!")
        print(f"ğŸ“ File saved")
    else:
        print("\nâŒ Analysis failed!")