import transformers
import pandas as pd
import glob
from datasets import Dataset
from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer
import os
import re
from sklearn.model_selection import train_test_split
import torch

device = torch.device("cpu")
model.to(device)

print(f"Transformers version: {transformers.__version__}")

# ====== 1. Clean text function for string format data ======
def clean_text(text):
    """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î text ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• string format"""
    if text is None or text == "":
        return ""
    
    text = str(text)
    
    # ‡πÅ‡∏õ‡∏•‡∏á newlines ‡πÄ‡∏õ‡πá‡∏ô spaces ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ BERT ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô
    text = text.replace('\n', ' ')
    text = text.replace('\r', ' ')
    
    # ‡∏•‡∏ö HTML tags ‡∏´‡∏≤‡∏Å‡∏°‡∏µ
    text = re.sub(r'<.*?>', '', text)
    
    # ‡∏•‡∏ö URLs ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏≠‡∏¢‡∏π‡πà
    text = re.sub(r'http\S+|www\S+', '', text)
    
    # ‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç ‡πÅ‡∏•‡∏∞ punctuation ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
    text = re.sub(r'[^\w\s.,!?:;-]', ' ', text)
    
    # Normalize whitespace
    text = re.sub(r'\s+', ' ', text)
    
    return text.strip()

# ====== 2. Load data from CSV files ======
def load_csv_data(file_path):
    """‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å CSV ‡πÅ‡∏•‡∏∞‡∏î‡∏∂‡∏á data_string column"""
    try:
        df = pd.read_csv(file_path, encoding='utf-8')
        if 'data_string' in df.columns:
            # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å data_string column
            return df['data_string'].iloc[0] if len(df) > 0 else ""
        else:
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ data_string column ‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏õ‡πá‡∏ô text
            return df.to_string()
    except Exception as e:
        print(f"‚ùå Error reading {file_path}: {e}")
        return ""

path_white = 'csv/white'
path_black = 'csv/black'

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ folders ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
if not os.path.exists(path_white):
    print(f"‚ùå White folder not found: {path_white}")
if not os.path.exists(path_black):
    print(f"‚ùå Black folder not found: {path_black}")

all_files_white = glob.glob(os.path.join(path_white, '*.csv'))
all_files_black = glob.glob(os.path.join(path_black, '*.csv'))

data_list = []

print("üü¢ Reading whitelist files...")
for filename in all_files_white:
    content = load_csv_data(filename)
    if content:
        cleaned = clean_text(content)
        if len(cleaned) > 10:  # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠
            data_list.append({
                'text': cleaned, 
                'label': 0,
                'source': os.path.basename(filename)
            })
            print(f"‚úÖ {os.path.basename(filename)} (white) - {len(cleaned)} chars")
        else:
            print(f"‚ö†Ô∏è  {os.path.basename(filename)} (white) - too short, skipped")

print("\nüî¥ Reading blacklist files...")
for filename in all_files_black:
    content = load_csv_data(filename)
    if content:
        cleaned = clean_text(content)
        if len(cleaned) > 10:  # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠
            data_list.append({
                'text': cleaned, 
                'label': 1,
                'source': os.path.basename(filename)
            })
            print(f"‚úÖ {os.path.basename(filename)} (black) - {len(cleaned)} chars")
        else:
            print(f"‚ö†Ô∏è  {os.path.basename(filename)} (black) - too short, skipped")

if not data_list:
    raise ValueError("‚ùå No valid data found. Check csv folders and data format.")

# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
combined_df = pd.DataFrame(data_list)

print(f"\nüìä Dataset Statistics:")
print(f"   ‚Ä¢ Total samples: {len(combined_df)}")
print(f"   ‚Ä¢ White samples: {len(combined_df[combined_df['label'] == 0])}")
print(f"   ‚Ä¢ Black samples: {len(combined_df[combined_df['label'] == 1])}")
print(f"   ‚Ä¢ Average text length: {combined_df['text'].str.len().mean():.0f} chars")
print(f"   ‚Ä¢ Max text length: {combined_df['text'].str.len().max()} chars")
print(f"   ‚Ä¢ Min text length: {combined_df['text'].str.len().min()} chars")

# ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
print(f"\nüìã Sample data:")
for i, row in combined_df.head(2).iterrows():
    print(f"   {i+1}. {row['source']} (label: {row['label']})")
    print(f"      Text preview: {row['text'][:100]}...")

# ====== 3. Split data into train/validation ======
train_df, val_df = train_test_split(
    combined_df, 
    test_size=0.2, 
    stratify=combined_df['label'], 
    random_state=42
)

print(f"\nüîÑ Data Split:")
print(f"   ‚Ä¢ Training samples: {len(train_df)}")
print(f"   ‚Ä¢ Validation samples: {len(val_df)}")

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Hugging Face datasets
train_dataset = Dataset.from_pandas(train_df[['text', 'label']])
val_dataset = Dataset.from_pandas(val_df[['text', 'label']])

print(f"\n‚úÖ Hugging Face datasets created:")
print(f"   ‚Ä¢ Train: {train_dataset}")
print(f"   ‚Ä¢ Validation: {val_dataset}")

# ====== 4. Tokenization ======
model_name = "bert-base-uncased"
print(f"\nü§ñ Loading model: {model_name}")

tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)

def preprocess(examples):
    """Tokenize text with proper handling for long sequences"""
    texts = [str(t) if t is not None else "" for t in examples["text"]]
    
    # ‡πÉ‡∏ä‡πâ max_length ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    return tokenizer(
        texts, 
        truncation=True, 
        padding="max_length", 
        max_length=512,  # BERT's max length
        return_tensors=None
    )

print("üîß Tokenizing datasets...")
train_tokenized = train_dataset.map(preprocess, batched=True)
val_tokenized = val_dataset.map(preprocess, batched=True)

print("‚úÖ Tokenization completed")

# ====== 5. Training Arguments ======
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=8,  # ‡πÄ‡∏û‡∏¥‡πà‡∏° batch size
    per_device_eval_batch_size=8,
    num_train_epochs=5,  # ‡πÄ‡∏û‡∏¥‡πà‡∏° epochs
    evaluation_strategy="steps",  # evaluate ‡∏ó‡∏∏‡∏Å steps
    eval_steps=50,  # evaluate ‡∏ó‡∏∏‡∏Å 50 steps
    save_strategy="steps",
    save_steps=100,
    learning_rate=2e-5,  # learning rate ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
    weight_decay=0.01,  # regularization
    logging_dir="./logs",
    logging_steps=10,
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    warmup_steps=100,  # warmup
    report_to=None  # ‡πÑ‡∏°‡πà‡∏™‡πà‡∏á‡πÑ‡∏õ wandb
)

# ====== 6. Trainer ======
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_tokenized,
    eval_dataset=val_tokenized,
    tokenizer=tokenizer
)

print("\nüöÄ Starting model training...")
print("=" * 50)

# ‡πÄ‡∏£‡∏¥‡πà‡∏° training
trainer.train()

print("\n‚úÖ Training completed!")

# ====== 7. Evaluation ======
print("\nüìä Evaluating model...")
eval_results = trainer.evaluate()

print(f"üìà Evaluation Results:")
for key, value in eval_results.items():
    print(f"   ‚Ä¢ {key}: {value:.4f}")

# ====== 8. Save fine-tuned model ======
save_path = "website_classifier_model"
model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

print(f"\nüíæ Model saved to: {save_path}")

# ====== 9. Test model with sample prediction ======
print("\nüß™ Testing model with sample predictions...")

def predict_website_type(text):
    """‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå"""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = torch.softmax(outputs.logits, dim=-1)
        predicted_class = torch.argmax(predictions, dim=-1).item()
        confidence = predictions[0][predicted_class].item()
    
    return "Blacklist" if predicted_class == 1 else "Whitelist", confidence

# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
if len(val_df) > 0:
    sample = val_df.iloc[0]
    actual_label = "Blacklist" if sample['label'] == 1 else "Whitelist"
    
    # ‡∏ï‡πâ‡∏≠‡∏á import torch
    try:
        import torch
        predicted_label, confidence = predict_website_type(sample['text'][:500])
        print(f"   Sample from: {sample['source']}")
        print(f"   Actual: {actual_label}")
        print(f"   Predicted: {predicted_label} (confidence: {confidence:.3f})")
    except ImportError:
        print("   ‚ö†Ô∏è PyTorch not imported for testing")

# ====== 10. Final summary ======
print("\n" + "=" * 50)
print("üéâ TRAINING COMPLETE!")
print("=" * 50)
print(f"üì¶ Final Report:")
print(f"   ‚Ä¢ Total whitelist files: {len(all_files_white)}")
print(f"   ‚Ä¢ Total blacklist files: {len(all_files_black)}")
print(f"   ‚Ä¢ Total examples loaded: {len(data_list)}")
print(f"   ‚Ä¢ Training samples: {len(train_df)}")
print(f"   ‚Ä¢ Validation samples: {len(val_df)}")
print(f"   ‚Ä¢ Model saved to: {save_path}")
print("=" * 50)